Project wide:
    !Finished
    * Add RabbitMQ container as a message broker,
    !Finished
    * Add pytest, pytest-django, pytest-cov,
    !WIP...
    * Write tests for models before starting and integration of data coming from scrapers.
    
    * Start working on a tasker app. Tasker will integrate data coming from a scrapers with django models.
    ...
    * Integrate rapport-Handler app that will process data from database and create XLSX rapports.
    * Rapport should be send daily/monthly/yearly via email.
    * Learn how to test Scrapers...

Scraper:
    !Finished
    * Add scrape metadata method. that will be called on every requested page.
    should return values: 
        - discovery_url,
        - current_url,
        - meta_author,
        - meta_description,
        - canonical_url,
        - seo_title,

    !Finished
    * Add checking mechanism for validating if found URL exists, before actually calling it with Selenium.
    python get request that returns status codes:
        - returns status code upon which further scraping depends.

    !Finished
    * Add randomization mechanic for User Agents.
    each get request should use differen User Agent:
        - Add helper function that will randomly pick User Agent from a list,
        - Add settings.py that will hold important settings for Scrapers/Crawlers.


    * Add find_product_pages_for_all_pages
    Add method that will allow traversing a pagination of products based on URLS not Selenium:
        - find example page that doesn't requires Selenium to go through pages,


    * Add displaying of total number of product pages data (pagination)
    find_product_pages_for_all_pages_selenium should inform about total number of pages to process if this value is available:
        - Xpath for that is already covered in products_discovery_xpath_dict,
        - There should be information of total number of pages in logs,
        - Also this should allow to resume scraping from last scraped page,
            when for example process fails in the middle.
    
    * Add low level methods for interacting with different Selenium Web Elements. 
        This will be used to interact with ProductPages.
    methods ideas:
        - interact with dropdowns,
        - click on radials,
        - click on checkboxes,

    * Add method(s) for reacting to few Selenium Exceptions
    there are popups to close everywhere!:
        - create a dict of xpathses to close for some nasty web elements,
            that can appear randomly.
        - create a universal method,
            that will be initiated on ElementClickInterceptedException,
        - method should check web page for existance of xpathses,
            defined in mentioned dict and close found element.
        - scraping should proceed if blocking element was closed,
            and element is now clickable.
